{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore all the environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import ale_py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Max episode steps: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/charchit.sharma/miniconda3/envs/rl/lib/python3.12/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env_cartpole = gym.make('CartPole-v0')\n",
    "env_cartpole.action_space.seed(seed)\n",
    "\n",
    "# print some information\n",
    "print('Action space:', env_cartpole.action_space)\n",
    "print('Observation space:', env_cartpole.observation_space)\n",
    "print('Max episode steps:', env_cartpole.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Max episode steps: 1000\n"
     ]
    }
   ],
   "source": [
    "env_lunar_lander = gym.make('LunarLander-v3')\n",
    "env_lunar_lander.action_space.seed(seed)\n",
    "\n",
    "# print some information\n",
    "print('Action space:', env_lunar_lander.action_space)\n",
    "print('Observation space:', env_lunar_lander.observation_space)\n",
    "print('Max episode steps:', env_lunar_lander.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Max episode steps: 200\n"
     ]
    }
   ],
   "source": [
    "env_mountain_car = gym.make('MountainCar-v0')\n",
    "env_mountain_car.action_space.seed(seed)\n",
    "\n",
    "# print some information\n",
    "print('Action space:', env_mountain_car.action_space)\n",
    "print('Observation space:', env_mountain_car.observation_space)\n",
    "print('Max episode steps:', env_mountain_car.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space for pong is a bit unique, as each individual state is an image of the current state of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "Observation space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Max episode steps: None\n"
     ]
    }
   ],
   "source": [
    "env_pong = gym.make(\"ALE/Pong-v5\")\n",
    "env_pong.action_space.seed(seed)\n",
    "\n",
    "# print some information\n",
    "print('Action space:', env_pong.action_space)\n",
    "print('Observation space:', env_pong.observation_space)\n",
    "print('Max episode steps:', env_pong.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent to explore the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def select_action(self):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_trajectories=100, seed=42):\n",
    "    rewards_per_trajectory = []\n",
    "\n",
    "    for i in tqdm(range(n_trajectories)):\n",
    "        total_reward = 0\n",
    "        state, _ = env.reset(seed=seed + i)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards_per_trajectory.append(total_reward)\n",
    "\n",
    "    return rewards_per_trajectory\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_agent_1 = RandAgent(env_cartpole)\n",
    "rand_agent_2 = RandAgent(env_lunar_lander)\n",
    "rand_agent_3 = RandAgent(env_mountain_car)\n",
    "rand_agent_4 = RandAgent(env_pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1254.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for CartPole-v0: 25.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 133.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for LunarLander-v3: -174.82355641274222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 182.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for MountainCar-v0: -200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:34<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for ALE/Pong-v5: -20.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "for agent, env in [(rand_agent_1, env_cartpole), (rand_agent_2, env_lunar_lander), (rand_agent_3, env_mountain_car), (rand_agent_4, env_pong)]:\n",
    "    rewards = evaluate(agent, env)\n",
    "    all_rewards.append(rewards)\n",
    "    print(f'Average reward for {env.spec.id}: {sum(rewards) / len(rewards)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Source: [Info on environments](https://github.com/openai/gym/wiki/Leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the cartpole environment, a reward of +1 is provided for every timestep that the pole remains upright. The environment is considered solved when a reward of around 195 is obtained. The random agent clearly fails at this task, and is only able to keep the pole upright for around 25 timesteps.\n",
    "- In the lunar lander environment, firing the main engine costs -0.3, crashing costs -100, landing results in a reward of +100, and making touching the leg with the ground results in a reward of +10. The average reward of -170 clearly shows that the random agent almost always crashes.\n",
    "- In the mountaincar environment, a reward of -1 is provided for every timestep, until the car reaches the flag. This goes on for 200 timesteps. Clearly, the random agent never reaches the flag.\n",
    "- In the pong environment, a reward of +1 is provided for every point scored, and -1 for every point the opponent scores. The game is to 21. The random agent sometimes gets lucky, but usually loses by a huge margin.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
